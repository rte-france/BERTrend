services:
  # Embedding server service
  embedding_server:
    build:
      context: .
      dockerfile: Dockerfile.embedding_server
    image: bertrend-embedding-server:latest
    container_name: bertrend-embedding-server
    ports:
      - "6464:6464"  # Embedding server API
    volumes:
      # Mount HF_HOME directory to share with host
      - ${HF_HOME:-.cache/huggingface}:/app/.cache/huggingface
    environment:
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
      - CUDA_VISIBLE_DEVICES=0
      # User mapping environment variables
      - HOST_UID=${HOST_UID:-1000}
      - HOST_GID=${HOST_GID:-1000}
      # Ensure these are not empty strings
      - DEFAULT_RATE_LIMIT=${DEFAULT_RATE_LIMIT:-50}
      - DEFAULT_RATE_WINDOW=${DEFAULT_RATE_WINDOW:-60}
      # Set HF_HOME inside container
      - HF_HOME=/app/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "--fail", "--insecure", "https://localhost:6464/health", "||", "exit", "1"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 60s  # Longer start period as model loading can take time
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1  # Use 1 GPU for the embedding server
              capabilities: [gpu]

  # Main BERTrend application
  bertrend:
    build:
      context: .
      dockerfile: Dockerfile
    image: bertrend:latest
    container_name: bertrend
    depends_on:
      - embedding_server
    ports:
      - "8501:8501"  # Topic Analysis demo
      - "8502:8502"  # Weak Signals demo
      - "8503:8503"  # Prospective demo
    volumes:
      - ${BERTREND_BASE_DIR}:/bertrend
    environment:
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
      - CUDA_VISIBLE_DEVICES=0
      # User mapping environment variables
      - HOST_UID=${HOST_UID:-1000}
      - HOST_GID=${HOST_GID:-1000}
      # BERTrend base dir
      - BERTREND_BASE_DIR=/bertrend/
      # Configure to use the embedding server
      - EMBEDDING_SERVICE_URL=https://embedding_server:6464
      - EMBEDDING_SERVICE_USE_LOCAL=false
      # OpenAI environment variables (replace with your own values)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_ENDPOINT=${OPENAI_ENDPOINT:-}
      - OPENAI_DEFAULT_MODEL_NAME=${OPENAI_DEFAULT_MODEL_NAME:-gpt-4o-mini}
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8501/_stcore/health", "||", "curl", "--fail", "http://localhost:8502/_stcore/health", "||", "curl", "--fail", "http://localhost:8503/_stcore/health", "||", "exit", "1"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 5s
    restart: unless-stopped
    # Uncomment the lines below to enable GPU support
    deploy:
       resources:
         reservations:
           devices:
             - driver: nvidia
               count: 1  # Use 1 GPU for the bertrend server
               capabilities: [gpu]


# To run with GPU support, uncomment the deploy section above and use:
# docker-compose up -d

# To use environment variables from a .env file, create a .env file with:
# OPENAI_API_KEY=your_key
# OPENAI_ENDPOINT=your_endpoint
# OPENAI_DEFAULT_MODEL_NAME=your_model_name
# HF_HOME=/custom/path/to/huggingface/cache  # Optional: custom HF_HOME path
# HOST_UID=1000  # Optional: override default UID
# HOST_GID=1000  # Optional: override default GID
# Access the demos at:
# - Topic Analysis: http://localhost:8501
# - Weak Signals: http://localhost:8502
# - Prospective Demo: http://localhost:8503
