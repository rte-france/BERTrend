#  Copyright (c) 2024-2026, RTE (https://www.rte-france.com)
#  See AUTHORS.txt
#  SPDX-License-Identifier: MPL-2.0
#  This file is part of BERTrend.

import os
from datetime import datetime

from agents import Agent, Runner, WebSearchTool
from agents.run import RunConfig
from dotenv import load_dotenv
from loguru import logger
from pydantic import BaseModel, Field

from bertrend.bertrend_apps.data_provider.data_provider import DataProvider
from bertrend.bertrend_apps.data_provider.utils import wait

load_dotenv(override=True)

DEFAULT_MODEL = os.getenv("OPENAI_DEFAULT_MODEL", "gpt-4.1-mini")
DEFAULT_NUM_SUB_QUERIES = 5

# Disable tracing to avoid unnecessary overhead
_run_config = RunConfig(tracing_disabled=True)

# ---------------------------------------------------------------------------
# Pydantic models for structured agent outputs
# ---------------------------------------------------------------------------


class ResearchPlan(BaseModel):
    """Output of the planning step: a list of targeted sub-questions."""

    sub_queries: list[str] = Field(
        description="List of specific, targeted sub-questions to research"
    )


class SubQueryResult(BaseModel):
    """Output of a single research sub-query."""

    sub_query: str = Field(description="The sub-question that was researched")
    findings: str = Field(
        description="Detailed findings from web research on this sub-question"
    )
    source_urls: list[str] = Field(
        default_factory=list,
        description="URLs of sources consulted for this sub-query",
    )


class ResearchReport(BaseModel):
    """Output of the final synthesis step."""

    title: str = Field(description="A clear, descriptive title for the research report")
    text: str = Field(
        description="The full synthesized research report with structured sections"
    )
    summary: str = Field(description="A concise 2-3 sentence summary of key findings")
    source_urls: list[str] = Field(
        default_factory=list,
        description="All source URLs consulted across all sub-queries",
    )
    timestamp: str = Field(description="The current date in YYYY-MM-DD HH:MM:SS format")


# ---------------------------------------------------------------------------
# System prompts for each step
# ---------------------------------------------------------------------------

PLAN_PROMPT = """You are a research planning expert. Given a topic and date range, break it down into {num_sub_queries} specific, targeted sub-questions that together will provide comprehensive coverage of the topic.

Each sub-question should:
- Be specific enough to yield focused search results
- Cover a different angle or aspect of the topic
- Be answerable through web research

Return exactly {num_sub_queries} sub-questions."""

RESEARCH_PROMPT = """You are a web research specialist. Your task is to thoroughly search the web for information about a specific question and report your findings.

Instructions:
- Search for multiple sources to get a well-rounded view
- Include specific data points, statistics, dates, and names when available
- Note any conflicting information between sources
- Focus on information from the date range: {after} to {before}
- Report the URLs of all sources you consulted"""

SYNTHESIZE_PROMPT = """You are a senior research analyst. Your task is to synthesize multiple research findings into a single, comprehensive, well-structured report.

Instructions:
- Combine all sub-query findings into a coherent narrative
- Organize by themes, not by sub-query
- Include specific data points and statistics from the findings
- Note areas of consensus and any conflicting information
- Write in a professional, analytical tone
- Structure the report with clear sections
- The timestamp should be: {timestamp}
{language_instruction}"""


class DeepResearchProvider(DataProvider):
    """Data provider that uses the OpenAI Agents SDK WebSearchTool to perform
    multi-step deep web research and return synthesized research reports.

    The research process follows three steps:
    1. PLAN — Break the query into targeted sub-questions
    2. RESEARCH — Search the web for each sub-question independently
    3. SYNTHESIZE — Combine all findings into a coherent report
    """

    def __init__(
        self,
        model: str = None,
        search_context_size: str = "medium",
        num_sub_queries: int = DEFAULT_NUM_SUB_QUERIES,
    ):
        # Do NOT call super().__init__() — we don't need Goose3 article parser
        # since text is generated by the agent, not scraped from URLs.
        self.model = model or DEFAULT_MODEL
        self.search_context_size = search_context_size
        self.num_sub_queries = num_sub_queries

    # ------------------------------------------------------------------
    # Step 1: PLAN
    # ------------------------------------------------------------------

    def _plan(
        self, query: str, after: str, before: str, language: str = None
    ) -> list[str]:
        """Break the research query into targeted sub-questions."""
        agent = Agent(
            name="research_planner",
            model=self.model,
            instructions=PLAN_PROMPT.format(num_sub_queries=self.num_sub_queries),
            output_type=ResearchPlan,
        )

        prompt = (
            f"Topic: {query}\n"
            f"Date range: {after} to {before}\n"
            f"Current date: {datetime.now().strftime('%Y-%m-%d')}"
        )
        if language:
            prompt += f"\nGenerate sub-questions in {language}."

        result = Runner.run_sync(agent, input=prompt, run_config=_run_config)
        plan: ResearchPlan = result.final_output
        return plan.sub_queries

    # ------------------------------------------------------------------
    # Step 2: RESEARCH each sub-question
    # ------------------------------------------------------------------

    def _research_sub_query(
        self, sub_query: str, after: str, before: str
    ) -> SubQueryResult:
        """Search the web for a single sub-question and return findings."""
        agent = Agent(
            name="web_researcher",
            model=self.model,
            instructions=RESEARCH_PROMPT.format(after=after, before=before),
            tools=[WebSearchTool(search_context_size=self.search_context_size)],
            output_type=SubQueryResult,
        )

        result = Runner.run_sync(agent, input=sub_query, run_config=_run_config)
        return result.final_output

    # ------------------------------------------------------------------
    # Step 3: SYNTHESIZE all findings
    # ------------------------------------------------------------------

    def _synthesize(
        self, query: str, findings: list[SubQueryResult], language: str = None
    ) -> ResearchReport:
        """Combine all sub-query findings into a single coherent report."""
        language_instruction = f"Write the report in {language}." if language else ""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        agent = Agent(
            name="research_synthesizer",
            model=self.model,
            instructions=SYNTHESIZE_PROMPT.format(
                timestamp=timestamp,
                language_instruction=language_instruction,
            ),
            output_type=ResearchReport,
        )

        # Build the input with all findings
        findings_text = f"Original research topic: {query}\n\n"
        for i, f in enumerate(findings, 1):
            findings_text += f"--- Sub-query {i}: {f.sub_query} ---\n"
            findings_text += f"{f.findings}\n"
            if f.source_urls:
                findings_text += f"Sources: {', '.join(f.source_urls)}\n"
            findings_text += "\n"

        result = Runner.run_sync(agent, input=findings_text, run_config=_run_config)
        return result.final_output

    # ------------------------------------------------------------------
    # Main entry point (DataProvider interface)
    # ------------------------------------------------------------------

    @wait(2)
    def get_articles(
        self,
        query: str,
        after: str,
        before: str,
        max_results: int,
        language: str = None,
    ) -> list[dict]:
        """Perform multi-step deep web research and return synthesized reports.

        The process follows three steps:
        1. PLAN — Break the query into targeted sub-questions
        2. RESEARCH — Search the web for each sub-question
        3. SYNTHESIZE — Combine all findings into a coherent report

        Parameters
        ----------
        query : str
            Keywords or topic to research.
        after : str
            Start date formatted as YYYY-MM-DD.
        before : str
            End date formatted as YYYY-MM-DD.
        max_results : int
            Maximum number of research reports to generate.
        language : str, optional
            Language hint for the report output.

        Returns
        -------
        list[dict]
            A list of article dicts with keys: title, text, summary, url,
            link, timestamp.
        """
        logger.info(
            f"DeepResearchProvider: starting multi-step research for '{query}' "
            f"(date range: {after} to {before}, max_results: {max_results})"
        )

        articles = []
        for i in range(max_results):
            try:
                article = self._run_research_pipeline(query, after, before, language)
                if article is not None:
                    articles.append(article)
                    logger.info(
                        f"DeepResearchProvider: report {i + 1}/{max_results} complete: "
                        f"'{article['title']}'"
                    )
            except Exception as e:
                logger.error(
                    f"DeepResearchProvider: error in report {i + 1}/{max_results} "
                    f"for query '{query}': {e}"
                )
                continue

        logger.info(
            f"DeepResearchProvider: completed with {len(articles)} report(s) "
            f"for query '{query}'"
        )
        return articles

    def _run_research_pipeline(
        self, query: str, after: str, before: str, language: str = None
    ) -> dict | None:
        """Execute the full PLAN → RESEARCH → SYNTHESIZE pipeline for one report."""

        # Step 1: PLAN
        logger.info("[PLAN] Breaking query into sub-questions...")
        sub_queries = self._plan(query, after, before, language)
        for j, sq in enumerate(sub_queries, 1):
            logger.info(f"  → Sub-question {j}/{len(sub_queries)}: {sq}")

        # Step 2: RESEARCH each sub-question
        findings: list[SubQueryResult] = []
        for j, sq in enumerate(sub_queries, 1):
            logger.info(f'[RESEARCH {j}/{len(sub_queries)}] Searching: "{sq}"')
            try:
                sub_result = self._research_sub_query(sq, after, before)
                findings.append(sub_result)
                n_sources = len(sub_result.source_urls)
                logger.info(f"  → Found {n_sources} source(s)")
            except Exception as e:
                logger.warning(f"  → Sub-query {j} failed: {e} (skipping)")
                continue

        if not findings:
            logger.error("DeepResearchProvider: all sub-queries failed, no findings")
            return None

        # Step 3: SYNTHESIZE
        logger.info(
            f"[SYNTHESIZE] Combining {len(findings)} sub-query results into final report..."
        )
        report = self._synthesize(query, findings, language)

        return self._parse_entry(report)

    def _parse_entry(self, entry: ResearchReport) -> dict | None:
        """Convert a ResearchReport to the standard article dict format.

        Parameters
        ----------
        entry : ResearchReport
            The structured research report from the agent.

        Returns
        -------
        dict | None
            Article dict with keys: title, text, summary, url, link, timestamp.
            Returns None if the entry is invalid.
        """
        try:
            primary_url = entry.source_urls[0] if entry.source_urls else ""
            timestamp = entry.timestamp or datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            return {
                "title": entry.title,
                "text": entry.text,
                "summary": entry.summary,
                "url": primary_url,
                "link": primary_url,
                "timestamp": timestamp,
            }
        except Exception as e:
            logger.error(f"DeepResearchProvider: error parsing report: {e}")
            return None
